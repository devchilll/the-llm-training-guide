# Stage 0: Pre-Training Fundamentals

> **Goal**: Understand training dynamics by building a language model from scratch.

## Labs in this directory

| File | Description |
|------|-------------|
| `nanogpt_from_scratch.ipynb` | Train a character-level LM on Shakespeare |
| `attention_implementation.py` | Hand-coded multi-head attention |

## Getting Started

1. Clone [nanoGPT](https://github.com/karpathy/nanoGPT)
2. Watch [Karpathy's "Let's build GPT"](https://www.youtube.com/watch?v=kCc8FmEb1nY)
3. Follow the notebook to train your first model

## Success Criteria

- [ ] Model generates text after 1000 steps
- [ ] Loss decreases smoothly without NaN
- [ ] You can explain every line of the training loop
